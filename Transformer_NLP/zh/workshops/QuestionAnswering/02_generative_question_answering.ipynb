{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "901d0bee",
   "metadata": {},
   "source": [
    "<center><a href=\"https://5loi.com/about_loi\"> <img src=\"images/DLI_Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501e5e7e",
   "metadata": {
    "id": "tiIOhb7iVC3J"
   },
   "source": [
    "# 概述\n",
    "> Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2fbc29d",
   "metadata": {
    "id": "PucJwfbhVC3L"
   },
   "source": [
    "## 任务描述\n",
    "> Task Description\n",
    "\n",
    "- 给定一段上下文和一个自然语言查询，我们希望为该查询生成一个答案\n",
    "- 根据答案的生成方式，任务可以大致分为两种类型：\n",
    "    1. 抽取式问答(Extractive Question Answering)\n",
    "    2. **生成式问答**(Generative Question Answering)\n",
    "\n",
    "\n",
    "### 使用 S2S 和 GPT 类模型进行生成式问答\n",
    "> Generative Question-Answering with S2S and GPT-like models\n",
    "\n",
    "给定一个问题和一段上下文，两者都使用自然语言，为该问题生成一个答案。与 BERT 类模型不同，答案不需要局限于上下文中的某个片段。\n",
    "\n",
    "Given a question and a context, both in natural language, generate an answer for the question. Unlike the BERT-like models, there is no constraint that the answer should be a span within the context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec940534",
   "metadata": {
    "id": "_xQBtr0KVC3M"
   },
   "outputs": [],
   "source": [
    "BRANCH = 'main'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829cb1c5",
   "metadata": {
    "id": "fof5-57iVC3N"
   },
   "source": [
    "# 导入和常量\n",
    "> Imports and constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92dfc336",
   "metadata": {
    "id": "KqKD-wReVC3O"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import wget\n",
    "import gc\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "from nemo.collections.nlp.models.question_answering.qa_gpt_model import GPTQAModel\n",
    "from nemo.collections.nlp.models.question_answering.qa_s2s_model import S2SQAModel\n",
    "\n",
    "gc.disable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be31d7a",
   "metadata": {
    "id": "xhPr9Jf_VC3O"
   },
   "outputs": [],
   "source": [
    "# set the following paths\n",
    "DATA_DIR = \"data\" # directory for storing datasets\n",
    "WORK_DIR = \"work_dir\" # directory for storing trained models, logs, additionally downloaded scripts\n",
    "\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "os.makedirs(WORK_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4c0011",
   "metadata": {
    "id": "dWymW8e0VC3O"
   },
   "source": [
    "# 配置\n",
    "> Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a293dca",
   "metadata": {
    "id": "0YhKTkuXVC3P"
   },
   "source": [
    "模型在配置文件中定义，该文件声明了多个重要部分：\n",
    "- **model**: 所有与模型相关的参数 - 语言模型、跨度预测、优化器和调度器、数据集以及任何其他相关信息\n",
    "- **trainer**: 传递给 PyTorch Lightning 的任何参数\n",
    "- **exp_manager**: 用于设置实验管理器的所有参数 - 目标目录、名称、日志信息\n",
    "\n",
    "我们将下载在 `NeMo/examples/nlp/question_answering/conf/qa_conf.yaml` 中提供的默认配置文件，并编辑训练不同模型所需的必要值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b63b34d",
   "metadata": {
    "id": "WOIWJqQ0VC3P"
   },
   "outputs": [],
   "source": [
    "# download the model's default configuration file \n",
    "config_dir = WORK_DIR + '/conf/'\n",
    "os.makedirs(config_dir, exist_ok=True)\n",
    "if not os.path.exists(config_dir + \"qa_conf.yaml\"):\n",
    "    print('Downloading config file...')\n",
    "    wget.download(f'https://raw.githubusercontent.com/NVIDIA/NeMo/{BRANCH}/examples/nlp/question_answering/conf/qa_conf.yaml', config_dir)\n",
    "else:\n",
    "    print ('config file already exists')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4280307",
   "metadata": {
    "id": "cvD-gv-FVC3P",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# this will print the entire default config of the model\n",
    "config_path = f'{WORK_DIR}/conf/qa_conf.yaml'\n",
    "print(config_path)\n",
    "config = OmegaConf.load(config_path)\n",
    "print(\"Default Config - \\n\")\n",
    "print(OmegaConf.to_yaml(config))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b19f30",
   "metadata": {
    "id": "E08e-ItPVC3P"
   },
   "source": [
    "# 在 SQuAD v2.0 上训练和测试模型\n",
    "> Training and testing models on SQuAD v2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b2ce7d",
   "metadata": {
    "id": "xn022MsKVC3Q"
   },
   "source": [
    "## 数据集\n",
    "> Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21cd31ab",
   "metadata": {
    "id": "c356CGL1VC3Q"
   },
   "source": [
    "在本例中，我们将下载 [SQuAD](https://rajpurkar.github.io/SQuAD-explorer/) 数据集以展示如何进行训练和推理。有两个数据集，SQuAD1.0 和 SQuAD2.0。SQuAD 1.1（SQuAD 数据集的先前版本）包含 100,000 多个关于 500 多篇文章的问答对。SQuAD2.0 数据集将 SQuAD1.1 中的 100,000 个问题与由众包工作者对抗性编写的 50,000 多个无法回答的问题相结合，使其看起来类似于可回答的问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133dde4d",
   "metadata": {
    "id": "nprGkyvRVC3Q"
   },
   "source": [
    "我们已经准备了数据目录 \"squad\"，其中包含以下四个文件，用于训练和评估：\n",
    "\n",
    "\n",
    "```\n",
    "squad  \n",
    "│\n",
    "└───v1.1\n",
    "│   │ -  train-v1.1.json\n",
    "│   │ -  dev-v1.1.json\n",
    "│\n",
    "└───v2.0\n",
    "    │ -  train-v2.0.json\n",
    "    │ -  dev-v2.0.json\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8486b2",
   "metadata": {
    "id": "GX0KWQXKVC3Q"
   },
   "outputs": [],
   "source": [
    "!ls -LR {DATA_DIR}/squad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64cb2fd9",
   "metadata": {
    "id": "RFVcvseOVC3R"
   },
   "source": [
    "## 设置数据集配置值\n",
    "> Set dataset config values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26cf79a4",
   "metadata": {
    "id": "Grb0EeRqVC3R"
   },
   "outputs": [],
   "source": [
    "# if True, model will load features from cache if file is present, or\n",
    "# create features and dump to cache file if not already present\n",
    "config.model.dataset.use_cache = False\n",
    "\n",
    "# indicates whether the dataset has unanswerable questions\n",
    "config.model.dataset.version_2_with_negative = True\n",
    "\n",
    "# indicates whether the dataset is of extractive nature or not\n",
    "# if True, context spans/chunks that do not contain answer are treated as unanswerable \n",
    "config.model.dataset.check_if_answer_in_context = True\n",
    "\n",
    "# set file paths for train, validation, and test datasets\n",
    "config.model.train_ds.file = f\"{DATA_DIR}/squad/v2.0/train-v2.0.json\"\n",
    "config.model.validation_ds.file = f\"{DATA_DIR}/squad/v2.0/dev-v2.0.json\"\n",
    "config.model.test_ds.file = f\"{DATA_DIR}/squad/v2.0/dev-v2.0.json\"\n",
    "\n",
    "# set batch sizes for train, validation, and test datasets\n",
    "config.model.train_ds.batch_size = 8\n",
    "config.model.validation_ds.batch_size = 8\n",
    "config.model.test_ds.batch_size = 8\n",
    "\n",
    "# set number of samples to be used from dataset. setting to -1 uses entire dataset\n",
    "config.model.train_ds.num_samples = 5000\n",
    "config.model.validation_ds.num_samples = 1000\n",
    "config.model.test_ds.num_samples = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1944d0",
   "metadata": {
    "id": "rFWF41VwVC3R"
   },
   "source": [
    "## 设置训练器配置值\n",
    "> Set trainer config values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f106d0d4",
   "metadata": {
    "id": "42yif-GIVC3R"
   },
   "outputs": [],
   "source": [
    "config.trainer.max_epochs = 1\n",
    "config.trainer.max_steps = -1 # takes precedence over max_epochs\n",
    "config.trainer.precision = 16\n",
    "config.trainer.devices = [0] # 0 for CPU, or list of the GPUs to use [0] this tutorial does not support multiple GPUs. If needed please use NeMo/examples/nlp/question_answering/question_answering.py\n",
    "config.trainer.accelerator = \"gpu\"\n",
    "config.trainer.strategy=\"auto\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1970aa0",
   "metadata": {
    "id": "EDQzMBlbVC3R"
   },
   "source": [
    "## 设置实验管理器配置值\n",
    "> Set experiment manager config values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5be1df",
   "metadata": {
    "id": "pxY4rnJBVC3R"
   },
   "outputs": [],
   "source": [
    "# config.exp_manager.exp_dir = WORK_DIR\n",
    "# config.exp_manager.name = \"QA-SQuAD2\"\n",
    "# config.exp_manager.create_wandb_logger=False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e09e85",
   "metadata": {
    "id": "zyh0SNiyVC3S"
   },
   "source": [
    "## 用于 SQuAD v2.0 的 S2S BART 模型\n",
    "> S2S BART model for SQuAD v2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032423d4",
   "metadata": {
    "id": "Sy9IYgVYVC3S"
   },
   "source": [
    "### 设置模型配置值\n",
    "> Set model config values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ba90f7",
   "metadata": {
    "id": "PKNmHKV5VC3S"
   },
   "outputs": [],
   "source": [
    "# set language model and tokenizer to be used\n",
    "# tokenizer is derived from model if a tokenizer name is not provided\n",
    "config.model.language_model.pretrained_model_name = \"facebook/bart-base\"\n",
    "config.model.tokenizer.tokenizer_name = \"facebook/bart-base\"\n",
    "\n",
    "# path where model will be saved\n",
    "config.model.nemo_path = f\"{WORK_DIR}/checkpoints/bart_squad_v2_0.nemo\"\n",
    "\n",
    "config.exp_manager.create_checkpoint_callback = True\n",
    "\n",
    "config.model.optim.lr = 5e-5\n",
    "\n",
    "#remove vocab_file from gpt model\n",
    "config.model.tokenizer.vocab_file = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f587f28",
   "metadata": {
    "id": "S_0glS4yVC3S"
   },
   "source": [
    "### 创建训练器并初始化模型\n",
    "> Create trainer and initialize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1dfed2f",
   "metadata": {
    "id": "8jWyHY1oVC3S",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# uncomment below line and run if you get an error while initializing tokenizer on Colab (reference: https://github.com/huggingface/transformers/issues/8690)\n",
    "# !rm -r /root/.cache/huggingface/\n",
    "\n",
    "trainer = pl.Trainer(**config.trainer)\n",
    "model = S2SQAModel(config.model, trainer=trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040660ab",
   "metadata": {
    "id": "xg-j39b4VC3S"
   },
   "source": [
    "### 训练、测试和保存模型\n",
    "> Train, test, and save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1a3f48",
   "metadata": {
    "id": "ocsf0EBDVC3S",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer.fit(model)\n",
    "trainer.test(model)\n",
    "\n",
    "model.save_to(config.model.nemo_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dcb9482",
   "metadata": {
    "id": "Vs3pl0VMVC3S"
   },
   "source": [
    "### 加载保存的模型并运行推理\n",
    "> Load the saved model and run inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7764759",
   "metadata": {
    "id": "NoW6_GO_VC3S",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = S2SQAModel.restore_from(config.model.nemo_path)\n",
    "\n",
    "eval_device = [config.trainer.devices[0]] if isinstance(config.trainer.devices, list) else 1\n",
    "model.trainer = pl.Trainer(\n",
    "    devices=eval_device,\n",
    "    accelerator=config.trainer.accelerator,\n",
    "    precision=16,\n",
    "    logger=False,\n",
    ")\n",
    "\n",
    "all_preds, all_nbest = model.inference(\n",
    "    config.model.test_ds.file,\n",
    "#     output_prediction_file=output_prediction_file,\n",
    "#     output_nbest_file=output_nbest_file,\n",
    "    num_samples=10, # setting to -1 will use all samples for inference\n",
    ")\n",
    "\n",
    "for question_id in all_preds:\n",
    "    print(all_preds[question_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dcde4b6",
   "metadata": {
    "id": "a7-iInbPVC3S"
   },
   "source": [
    "## 用于 SQuAD v2.0 的 GPT2 模型\n",
    "> GPT2 model for SQuAD v2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fcf79ea",
   "metadata": {
    "id": "VaIC0l2aVC3S"
   },
   "source": [
    "### 练习 # 1 - 设置模型配置值\n",
    "> Exercise # 1 - Set model config values\n",
    "\n",
    "* 修改 `<FIXME>` 以使用 `gpt2` 预训练模型和分词器。\n",
    "* Modify the `<FIXME>` to use the `gpt2` pre-trained model and tokenizer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ee2e4d",
   "metadata": {
    "id": "5j6SVk6fVC3S"
   },
   "outputs": [],
   "source": [
    "# set language model and tokenizer to be used\n",
    "# tokenizer is derived from model if a tokenizer name is not provided\n",
    "config.model.language_model.pretrained_model_name = <<<<FIXME>>>>\n",
    "config.model.tokenizer.tokenizer_name = <<<<FIXME>>>>\n",
    "\n",
    "# path where model will be saved\n",
    "config.model.nemo_path = f\"{WORK_DIR}/checkpoints/gpt2_squad_v2_0.nemo\"\n",
    "\n",
    "config.exp_manager.create_checkpoint_callback = True\n",
    "\n",
    "config.model.optim.lr = 1e-4"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d2378d07",
   "metadata": {
    "id": "5j6SVk6fVC3S",
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "# set language model and tokenizer to be used\n",
    "# tokenizer is derived from model if a tokenizer name is not provided\n",
    "config.model.language_model.pretrained_model_name = \"gpt2\"\n",
    "config.model.tokenizer.tokenizer_name = \"gpt2\"\n",
    "\n",
    "# path where model will be saved\n",
    "config.model.nemo_path = f\"{WORK_DIR}/checkpoints/gpt2_squad_v2_0.nemo\"\n",
    "\n",
    "config.exp_manager.create_checkpoint_callback = True\n",
    "\n",
    "config.model.optim.lr = 1e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833ab1a2",
   "metadata": {
    "id": "rWhhEuvzVC3S"
   },
   "source": [
    "### 创建训练器并初始化模型\n",
    "> Create trainer and initialize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f128f70",
   "metadata": {
    "id": "vBtP3ukDVC3S",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer = pl.Trainer(**config.trainer)\n",
    "model = GPTQAModel(config.model, trainer=trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c0d6cf",
   "metadata": {
    "id": "EApFrJh8VC3T"
   },
   "source": [
    "### 练习 # 2 - 训练、测试和保存模型\n",
    "> Exercise # 2 - Train, test, and save the model\n",
    "\n",
    "* 修改 `<FIXME>` 以训练、测试和保存模型。\n",
    "* Modify the `<FIXME>` to train, test, and save the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827e1c97",
   "metadata": {
    "id": "zYo2JDdOVC3T",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "<<<<FIXME>>>>.fit(<<<<FIXME>>>>)\n",
    "<<<<FIXME>>>>.test(<<<<FIXME>>>>)\n",
    "\n",
    "<<<<FIXME>>>>.save_to(config.model.nemo_path)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8afc10b4",
   "metadata": {
    "id": "zYo2JDdOVC3T",
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "trainer.fit(model)\n",
    "trainer.test(model)\n",
    "\n",
    "model.save_to(config.model.nemo_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ded1494",
   "metadata": {
    "id": "6aNEt06fVC3T"
   },
   "source": [
    "### 练习 # 3 - 加载保存的模型并运行推理\n",
    "> Exercise # 3 - Load the saved model and run inference\n",
    "\n",
    "* 修改 `<FIXME>` 以从保存的模型运行推理。\n",
    "* Modify the `<FIXME>` to run inference from a saved model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1795e3f2",
   "metadata": {
    "id": "ioLT4DVbVC3T",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = GPTQAModel.restore_from(config.model.nemo_path)\n",
    "\n",
    "eval_device = [config.trainer.devices[0]] if isinstance(config.trainer.devices, list) else 1\n",
    "model.trainer = pl.Trainer(\n",
    "    devices=eval_device,\n",
    "    accelerator=config.trainer.accelerator,\n",
    "    precision=16,\n",
    "    logger=False,\n",
    ")\n",
    "\n",
    "all_preds, all_nbest = model.<<<<FIXME>>>>(\n",
    "    config.model.test_ds.file,\n",
    "    num_samples=10, # setting to -1 will use all samples for inference\n",
    ")\n",
    "\n",
    "for question_id in all_preds:\n",
    "    print(all_preds[question_id])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1f8adeac",
   "metadata": {
    "id": "ioLT4DVbVC3T",
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "model = GPTQAModel.restore_from(config.model.nemo_path)\n",
    "\n",
    "eval_device = [config.trainer.devices[0]] if isinstance(config.trainer.devices, list) else 1\n",
    "model.trainer = pl.Trainer(\n",
    "    devices=eval_device,\n",
    "    accelerator=config.trainer.accelerator,\n",
    "    precision=16,\n",
    "    logger=False,\n",
    ")\n",
    "\n",
    "all_preds, all_nbest = model.inference(\n",
    "    config.model.test_ds.file,\n",
    "    num_samples=10, # setting to -1 will use all samples for inference\n",
    ")\n",
    "\n",
    "for question_id in all_preds:\n",
    "    print(all_preds[question_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4987620",
   "metadata": {},
   "source": [
    "<center><a href=\"https://5loi.com/about_loi\"> <img src=\"images/DLI_Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a></center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
