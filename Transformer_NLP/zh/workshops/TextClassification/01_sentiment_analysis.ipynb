{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ac9f5e0",
   "metadata": {},
   "source": [
    "![DLI Header](images/DLI_Header.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca81ba90",
   "metadata": {},
   "source": [
    "# Text Classification #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451e39c7",
   "metadata": {},
   "source": [
    "## Sentimental Analysis ##\n",
    "\n",
    "In this notebook, you will learn to fine-tune a pre-trained model. Specifically, we will use a model for sentiment analysis. \n",
    "\n",
    "**Sentiment Analysis** is the task of detecting the sentiment in text. We model this problem as a simple form of a text classification problem. For example `Gollum's performance is incredible!` has a positive sentiment while `It's neither as romantic nor as thrilling as it should be.` has a negative sentiment. In such an analysis, we need to look at sentences, and we only have two classes: \"positive\" and \"negative\". Each sentence in the training set must be labeled as one or the other. Sentiment analysis is widely used by businesses to identify customer sentiment toward products, brands, or services in online conversations and feedback.\n",
    "\n",
    "**Table of Contents**<br>\n",
    "This notebook covers the below sections: \n",
    "* Dataset\n",
    "    * Download and Preprocess data\n",
    "    * Labeling Data (OPTIONAL)\n",
    "* Use Pre-Trained Model\n",
    "    * Download Model\n",
    "    * Make Predictions\n",
    "* Fine-Tune a Pre-Trained Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef24a1c",
   "metadata": {},
   "source": [
    "## Dataset ##\n",
    "\n",
    "In this notebook, we're going to use The [Stanford Sentiment Treebank (SST-2)](https://nlp.stanford.edu/sentiment/index.html) corpus for sentiment analysis. The data contains a collection of sentences with binary labels for positive and negative. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b608f11c",
   "metadata": {},
   "source": [
    "For text classification, NeMo requires the data to be in a specific format. Data needs to be in TAB separated files (.tsv) with two columns of sentence and label. Each line of the data file contains text sequences, where words are separated with spaces and label separated with [TAB], i.e.: `[WORD] [SPACE] [WORD] [SPACE] [WORD] [TAB] [LABEL]`\n",
    "\n",
    "For example: \n",
    "* \n",
    "```\n",
    "hide new secretions from the parental units[TAB]0\n",
    "that loves its characters and communicates something rather beautiful about human nature[TAB]1\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257b2c1e",
   "metadata": {},
   "source": [
    "### Download and Preprocess Data ###\n",
    "\n",
    "We have prepared the SST-2 dataset for you. It should contain three files of train.tsv, dev.tsv, and test.tsv which can be used for `training`, `validation`, and `test` respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d29a85",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import wget\n",
    "\n",
    "# set data path\n",
    "DATA_DIR='data'\n",
    "DATA_DIR=os.path.join(DATA_DIR, 'SST-2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea70c44",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# check that data folder should contain train.tsv, dev.tsv, test.tsv\n",
    "!ls -l {DATA_DIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07210d0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# preview data \n",
    "print('Train:')\n",
    "!head -n 5 {DATA_DIR}/train.tsv\n",
    "\n",
    "print('Dev:')\n",
    "!head -n 5 {DATA_DIR}/dev.tsv\n",
    "\n",
    "print('Test:')\n",
    "!head -n 5 {DATA_DIR}/test.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e95d292",
   "metadata": {},
   "source": [
    "The format of `train.tsv` and `dev.tsv` is close to NeMo's format except to have an extra header line at the beginning of the files. We would remove these extra lines. But `test.tsv` has different format and labels are missing for this part of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37b31c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!sed 1d {DATA_DIR}/train.tsv > {DATA_DIR}/train_nemo_format.tsv\n",
    "!sed 1d {DATA_DIR}/dev.tsv > {DATA_DIR}/dev_nemo_format.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5e69a2",
   "metadata": {},
   "source": [
    "## Fine-Tune a Pre-Trained Model ##\n",
    "\n",
    "A text classification model is typically comprised of a pre-trained [BERT](https://arxiv.org/pdf/1810.04805.pdf) model followed by a text classification layer. For training, we can use a configuration file to define the model. The configuration (config) file consists of several important sections, including: \n",
    "* **model**: All arguments that are related to the Model - language model, token classifier, optimizer and schedulers, datasets and any other related information\n",
    "* **trainer**: Any argument to be passed to PyTorch Lightning\n",
    "\n",
    "_Note:_ NeMo provides a template for creating the configuration file, which is recommended as a starting point, but you can create your own as long as it follows the required format. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780d1645",
   "metadata": {},
   "source": [
    "### Configuration File ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1686e6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define config path\n",
    "MODEL_CONFIG=\"text_classification_config.yaml\"\n",
    "WORK_DIR='WORK_DIR'\n",
    "os.makedirs(WORK_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05975ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the model's configuration file \n",
    "BRANCH='main'\n",
    "config_dir = WORK_DIR + '/configs/'\n",
    "os.makedirs(config_dir, exist_ok=True)\n",
    "if not os.path.exists(config_dir + MODEL_CONFIG):\n",
    "    print('Downloading config file...')\n",
    "    wget.download(f'https://raw.githubusercontent.com/NVIDIA/NeMo/{BRANCH}/examples/nlp/text_classification/conf/' + MODEL_CONFIG, config_dir)\n",
    "else:\n",
    "    print ('config file already exists')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db62d648",
   "metadata": {},
   "source": [
    "The config file for text classification, `text_classification_config.yaml`, specifies model, training, and experiment management details, such as file locations, pre-trained models, and hyperparameters. The YAML config file we downloaded provides default values for most of the parameters, but there are a few items that must be specified for this experiment.\n",
    "\n",
    "Each YAML section is a bit easier to view using the `omegaconf` package, which allows you to access and manipulate the configuration keys using a \"dot\" notation. We'll take a look at the details of each section using the `OmegaConf` tool. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d7375e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from omegaconf import OmegaConf\n",
    "\n",
    "CONFIG_DIR = \"/dli/task/WORK_DIR/configs\"\n",
    "CONFIG_FILE = \"text_classification_config.yaml\"\n",
    "\n",
    "config=OmegaConf.load(CONFIG_DIR + \"/\" + CONFIG_FILE)\n",
    "\n",
    "# print the entire configuration file\n",
    "print(OmegaConf.to_yaml(config))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03275286",
   "metadata": {},
   "source": [
    "Notice that some config lines, including `model.dataset.data_dir`, have `???` in place of paths, this means that values for these fields are required to be specified by the user. Details about the model arguments can be found in the [documentation](https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/stable/nlp/text_classification.html#training-the-text-classification-model). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b3efb9",
   "metadata": {},
   "source": [
    "We first need to set the num_classes in the config file which specifies the number of classes in the dataset. For SST-2, we have just two classes (0-positive and 1-negative). So we set the num_classes to 2. The model supports more than 2 classes too.\n",
    "\n",
    "We need to specify and set the `model.train_ds.file_name`, `model.validation_ds.file_name`, and `model.test_ds.file_name` in the config file to the paths of the train, validation, and test files if they exist. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dee5325",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!ls $DATA_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd76f1d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# set num_classes to 2\n",
    "config.model.dataset.num_classes=2\n",
    "\n",
    "# set file paths\n",
    "config.model.train_ds.file_path = os.path.join(DATA_DIR, 'train_nemo_format.tsv')\n",
    "config.model.validation_ds.file_path = os.path.join(DATA_DIR, 'dev_nemo_format.tsv')\n",
    "\n",
    "# You may change other params like batch size or the number of samples to be considered (-1 means all the samples)\n",
    "\n",
    "# print the model section\n",
    "print(OmegaConf.to_yaml(config.model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1257bdc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(OmegaConf.to_yaml(config.trainer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66bc6afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets modify some trainer configs\n",
    "\n",
    "# setup max number of steps to reduce training time for demonstration purposes of this tutorial\n",
    "# Training stops when max_step or max_epochs is reached (earliest)\n",
    "config.trainer.max_epochs = 1\n",
    "\n",
    "# print the trainer section\n",
    "print(OmegaConf.to_yaml(config.trainer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97be14f",
   "metadata": {},
   "source": [
    "Note: `OmegaConf.to_yaml()` is used to create a proper format for printing the config. Once the `text_classification_config.yaml` file has been loaded into memory, changing the configuration file will require the config variable to be re-defined."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576e547b",
   "metadata": {},
   "source": [
    "Now, we are ready to initialize our model. During the model initialization call, the dataset and data loaders will be prepared for training and evaluation. Also, the pretrained BERT model will be downloaded, which can take up to a few minutes depending on the size of the chosen BERT model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad0f38d",
   "metadata": {},
   "source": [
    "### Download Pre-Trained Model ###\n",
    "\n",
    "Before initializing the model, we might want to modify some of the model configs. For example, we might want to modify the pretrained BERT model to another model. The default model is `bert-base-uncased`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724011a0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nemo.collections import nlp as nemo_nlp\n",
    "\n",
    "# complete list of supported BERT-like models\n",
    "for model in nemo_nlp.modules.get_pretrained_lm_models_list(): \n",
    "    print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50eb1333",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the BERT-like model, you want to use\n",
    "# set the `model.language_modelpretrained_model_name' parameter in the config to the model you want to use\n",
    "config.model.language_model.pretrained_model_name = \"bert-base-uncased\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da954cc",
   "metadata": {},
   "source": [
    "Now, we are ready to initialize our model. During the model initialization call, the dataset and data loaders will also be prepared for the training and validation.\n",
    "\n",
    "Also, the pretrained BERT model will be automatically downloaded. Note it can take up to a few minutes depending on the size of the chosen BERT model for the first time you create the model. If your dataset is large, it also may take some time to read and process all the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc06d707",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nemo.collections.nlp.models import TextClassificationModel\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "trainer=pl.Trainer(**config.trainer)\n",
    "text_classification_model=TextClassificationModel(cfg=config.model, trainer=trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ad93a7",
   "metadata": {},
   "source": [
    "### Model Training ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52be0f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# start model training\n",
    "trainer.fit(text_classification_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf97c14e",
   "metadata": {},
   "source": [
    "### Evaluate Predictions ###\n",
    "\n",
    "for inference, we can use `trainer.test()` or `model.classifytext()`. additional [documentation](https://github.com/NVIDIA/NeMo/blob/main/nemo/collections/nlp/models/text_classification/text_classification_model.py). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cde1c82",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "eval_config = OmegaConf.create({'file_path': config.model.validation_ds.file_path, 'batch_size': 64, 'shuffle': False, 'num_samples': -1})\n",
    "text_classification_model.setup_test_data(test_data_config=eval_config)\n",
    "trainer.test(model=text_classification_model, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48890987",
   "metadata": {},
   "source": [
    "### Inference ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26337d9e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# define the list of queries for inference\n",
    "queries = ['by the end of no such thing the audience , like beatrice , has a watchful affection for the monster .', \n",
    "           'director rob marshall went out gunning to make a great one .', \n",
    "           'uneasy mishmash of styles and genres .']\n",
    "           \n",
    "# max_seq_length=512 is the maximum length BERT supports.       \n",
    "results = text_classification_model.classifytext(queries=queries, batch_size=3, max_seq_length=512)\n",
    "\n",
    "print('The prediction results of some sample queries with the trained model:')\n",
    "for query, result in zip(queries, results):\n",
    "    print(f'Query : {query}')\n",
    "    print(f'Predicted label: {result}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8754d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# restart the kernel\n",
    "import IPython\n",
    "\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1aa666",
   "metadata": {},
   "source": [
    "**Well Done!** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b1d109",
   "metadata": {},
   "source": [
    "![DLI Header](images/DLI_Header.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
