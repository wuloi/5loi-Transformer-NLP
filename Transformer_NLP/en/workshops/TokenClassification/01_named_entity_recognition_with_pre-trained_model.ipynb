{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea42bdff",
   "metadata": {},
   "source": [
    "![DLI Header](images/DLI_Header.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456dcbf5",
   "metadata": {},
   "source": [
    "# Token Classification with Large Language Models #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f28cf3",
   "metadata": {},
   "source": [
    "## 01 - Named Entity Recognition with Pre-Trained Model ##\n",
    "\n",
    "In this notebook, you will learn to use a pre-trained token classification model. Specifically, we will use a model for named entity recognition. NER, also referred to as entity chunking, identification or extraction, is the task of detecting and classifying key information (entities) in text. In other words, a NER model takes a piece of text as input and for each word in the text, the model identifies a category the word belongs to. For example, in a sentence: `Mary lives in Santa Clara and works at NVIDIA`, the model should detect that `Mary` is a person, `Santa Clara` is a location and `NVIDIA` is a company.\n",
    "\n",
    "**Table of Contents**<br>\n",
    "This notebook covers the below sections: \n",
    "* Project Overview\n",
    "* Dataset\n",
    "    * Download and Preprocess data\n",
    "    * Labeling Data (OPTIONAL)\n",
    "* Use Pre-Trained Model\n",
    "    * Download Model\n",
    "    * Make Predictions\n",
    "    * Model Evaluation\n",
    "* Fine-Tune a Pre-Trained Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa6d0652",
   "metadata": {},
   "source": [
    "## Project Overview ##\n",
    "\n",
    "<img src='images/workflow.png' width=1080>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ad62d3",
   "metadata": {},
   "source": [
    "## Dataset ##\n",
    "For this notebook, we're going to use the [GMB (Groningen Meaning Bank)](http://www.let.rug.nl/bjerva/gmb/about.php) corpus for named entity recognition. GMB is a fairly large corpus with a lot of annotations. The data is labeled using the [IOB format](https://en.wikipedia.org/wiki/Inside%E2%80%93outside%E2%80%93beginning_(tagging)) (short for inside, outside, beginning), which means each annotation also needs a prefix of **I**, **O**, or **B**. \n",
    "\n",
    "The following classes appear in the dataset:\n",
    "* **LOC** - Geographical Entity\n",
    "* **ORG** - Organization\n",
    "* **PER** - Person\n",
    "* **GPE** - Geopolitical Entity\n",
    "* **TIME** - Time indicator\n",
    "* **ART** - Artifact\n",
    "* **EVE** - Event\n",
    "* **NAT** - Natural Phenomenon\n",
    "\n",
    "_Note:_ GMB is not completely human annotated, and itâ€™s not considered 100% correct. For this exercise, classes **ART**, **EVE**, and **NAT** were combined into a **MISC** class due to small number of examples for these classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68300249",
   "metadata": {},
   "source": [
    "For token classification tasks, NeMo requires the data to be in a specific format. Data needs to be split into  files: \n",
    "* `text.txt` and \n",
    "* `labels.txt`\n",
    "\n",
    "Each line of the **text.txt** file contains text sequences, where words are separated with spaces, i.e.: `[WORD] [SPACE] [WORD] [SPACE] [WORD]`. The **labels.txt** file contains corresponding labels for each word in **text.txt**, the labels are separated with spaces, i.e.: `[LABEL] [SPACE] [LABEL] [SPACE] [LABEL]`.\n",
    "\n",
    "For example: \n",
    "* **text.txt**\n",
    "```\n",
    "Jennifer is from New York City .\n",
    "She likes ...\n",
    "...\n",
    "```\n",
    "* **labels.txt**\n",
    "```\n",
    "B-PER O O B-LOC I-LOC I-LOC O\n",
    "O O ...\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cff788f",
   "metadata": {},
   "source": [
    "### Download and Preprocess Data ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "688aa1f6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install wget\n",
    "import os\n",
    "import wget\n",
    "\n",
    "# set data path\n",
    "DATA_DIR=\"data/GMB\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "03eb714a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 11140\n",
      "-rw-r--r-- 1 kappa kappa      77 Aug 19 15:39 label_ids.csv\n",
      "-rw-r--r-- 1 kappa kappa  407442 Aug 19 15:39 labels_dev.txt\n",
      "-rw-r--r-- 1 kappa kappa 3169783 Aug 19 15:40 labels_train.txt\n",
      "-rw-r--r-- 1 kappa kappa  891020 Aug 19 15:40 text_dev.txt\n",
      "-rw-r--r-- 1 kappa kappa 6928251 Aug 19 15:40 text_train.txt\n"
     ]
    }
   ],
   "source": [
    "# check that data folder should contain 4 files\n",
    "!ls -l $DATA_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0684f72f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text:\n",
      "New Zealand 's cricket team has scored a morale-boosting win over Bangladesh in the first of three one-day internationals in New Zealand .\n",
      "Despite Bangladesh 's highest total ever in a limited-overs match , the Kiwis were able to win the match by six wickets in Auckland .\n",
      "Opening batsman Jamie How led all scorers with 88 runs as New Zealand reached 203-4 in 42.1 overs .\n",
      "The score was in response to Bangladesh 's total of 201 all out in 46.3 overs .\n",
      "Mohammad Ashraful led the visitors with 70 runs , including 10 fours and one six on the short boundaries of the Eden Park ground .\n",
      "Labels:\n",
      "B-LOC I-LOC O O O O O O O O O B-LOC O O B-TIME I-TIME I-TIME I-TIME O O B-LOC I-LOC O\n",
      "O B-LOC O O O O O O O O O O B-GPE O O O O O O O O O O B-LOC O\n",
      "O O B-PER I-PER O O O O O O O B-LOC I-LOC O O O O O O\n",
      "O O O O O O B-LOC O O O O O O O O O O\n",
      "B-PER I-PER O O O O O O O O O O O O O O O O O O O B-LOC I-LOC O O\n"
     ]
    }
   ],
   "source": [
    "# preview data \n",
    "print('Text:')\n",
    "!head -n 5 {DATA_DIR}/text_train.txt\n",
    "\n",
    "print('Labels:')\n",
    "!head -n 5 {DATA_DIR}/labels_train.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f0dff5",
   "metadata": {},
   "source": [
    "### Labeling Data ###\n",
    "\n",
    "If you have raw data, NeMo recommends using the [Datasaur](https://datasaur.ai/) labeling platform to apply labels to data. Datasaur was designed specifically for labeling text data and supports basic NLP labeling tasks such as Named Entity Recognition and text classification through advanced NLP tasks such as dependency parsing and coreference resolution. You can sign up for Datasaur for free at https://datasaur.ai/sign-up/. Once you upload a file, you can choose from multiple NLP project types and use the Datasaur interface to label the data. After labeling, you can export the labeled data using the conll_2003 format, which integrates directly with NeMo. A video walkthrough can be found [here](https://www.youtube.com/watch?v=I9WVmnnSciE)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1735127d",
   "metadata": {},
   "source": [
    "## Use Pre-Trained Model ##\n",
    "NeMo supports NER and other token-level classification tasks. These models typically comprise of a pre-trained [BERT](https://arxiv.org/pdf/1810.04805.pdf) model followed by a token classification layer. We start by using a pre-trained model. The `TokenClassificationModel` inherits from `NLPModel` and has the below methods: \n",
    "* `TokenClassificationModel.add_predictions()`\n",
    "* `TokenClassificationModel.evaluate_from_file()`\n",
    "\n",
    "These are useful when making inference and evaluating model performance. Additional functionality of the `TokenClassificationModel` can be found in the [source code](https://github.com/NVIDIA/NeMo/blob/main/nemo/collections/nlp/models/token_classification/token_classification_model.py). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a98fc74",
   "metadata": {},
   "source": [
    "### Download Pre-Trained Model ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "683b9c6b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: apex in ./.venv/lib/python3.10/site-packages (0.9.10.dev0)\n",
      "Requirement already satisfied: cryptacular in ./.venv/lib/python3.10/site-packages (from apex) (1.6.2)\n",
      "Requirement already satisfied: zope.sqlalchemy in ./.venv/lib/python3.10/site-packages (from apex) (3.1)\n",
      "Requirement already satisfied: velruse>=1.0.3 in ./.venv/lib/python3.10/site-packages (from apex) (1.1.1)\n",
      "Requirement already satisfied: pyramid>1.1.2 in ./.venv/lib/python3.10/site-packages (from apex) (2.0.2)\n",
      "Requirement already satisfied: pyramid-mailer in ./.venv/lib/python3.10/site-packages (from apex) (0.15.1)\n",
      "Requirement already satisfied: requests in ./.venv/lib/python3.10/site-packages (from apex) (2.32.3)\n",
      "Requirement already satisfied: wtforms in ./.venv/lib/python3.10/site-packages (from apex) (3.1.2)\n",
      "Requirement already satisfied: wtforms-recaptcha in ./.venv/lib/python3.10/site-packages (from apex) (0.3.2)\n",
      "Requirement already satisfied: hupper>=1.5 in ./.venv/lib/python3.10/site-packages (from pyramid>1.1.2->apex) (1.12.1)\n",
      "Requirement already satisfied: plaster in ./.venv/lib/python3.10/site-packages (from pyramid>1.1.2->apex) (1.1.2)\n",
      "Requirement already satisfied: plaster-pastedeploy in ./.venv/lib/python3.10/site-packages (from pyramid>1.1.2->apex) (1.0.1)\n",
      "Requirement already satisfied: setuptools in ./.venv/lib/python3.10/site-packages (from pyramid>1.1.2->apex) (72.2.0)\n",
      "Requirement already satisfied: translationstring>=0.4 in ./.venv/lib/python3.10/site-packages (from pyramid>1.1.2->apex) (1.4)\n",
      "Requirement already satisfied: venusian>=1.0 in ./.venv/lib/python3.10/site-packages (from pyramid>1.1.2->apex) (3.1.0)\n",
      "Requirement already satisfied: webob>=1.8.3 in ./.venv/lib/python3.10/site-packages (from pyramid>1.1.2->apex) (1.8.8)\n",
      "Requirement already satisfied: zope.deprecation>=3.5.0 in ./.venv/lib/python3.10/site-packages (from pyramid>1.1.2->apex) (5.0)\n",
      "Requirement already satisfied: zope.interface>=3.8.0 in ./.venv/lib/python3.10/site-packages (from pyramid>1.1.2->apex) (7.0.1)\n",
      "Requirement already satisfied: requests-oauthlib in ./.venv/lib/python3.10/site-packages (from velruse>=1.0.3->apex) (2.0.0)\n",
      "Requirement already satisfied: anykeystore in ./.venv/lib/python3.10/site-packages (from velruse>=1.0.3->apex) (0.2)\n",
      "Requirement already satisfied: python3-openid in ./.venv/lib/python3.10/site-packages (from velruse>=1.0.3->apex) (3.2.0)\n",
      "Requirement already satisfied: pbkdf2 in ./.venv/lib/python3.10/site-packages (from cryptacular->apex) (1.3)\n",
      "Requirement already satisfied: repoze.sendmail>=4.1 in ./.venv/lib/python3.10/site-packages (from pyramid-mailer->apex) (4.4.1)\n",
      "Requirement already satisfied: transaction in ./.venv/lib/python3.10/site-packages (from pyramid-mailer->apex) (4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.10/site-packages (from requests->apex) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.10/site-packages (from requests->apex) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.10/site-packages (from requests->apex) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.10/site-packages (from requests->apex) (2024.7.4)\n",
      "Requirement already satisfied: markupsafe in ./.venv/lib/python3.10/site-packages (from wtforms->apex) (2.1.5)\n",
      "Requirement already satisfied: packaging in ./.venv/lib/python3.10/site-packages (from zope.sqlalchemy->apex) (24.1)\n",
      "Requirement already satisfied: SQLAlchemy!=1.4.0,!=1.4.1,!=1.4.2,!=1.4.3,!=1.4.4,!=1.4.5,!=1.4.6,>=1.1 in ./.venv/lib/python3.10/site-packages (from zope.sqlalchemy->apex) (2.0.32)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in ./.venv/lib/python3.10/site-packages (from SQLAlchemy!=1.4.0,!=1.4.1,!=1.4.2,!=1.4.3,!=1.4.4,!=1.4.5,!=1.4.6,>=1.1->zope.sqlalchemy->apex) (4.12.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in ./.venv/lib/python3.10/site-packages (from SQLAlchemy!=1.4.0,!=1.4.1,!=1.4.2,!=1.4.3,!=1.4.4,!=1.4.5,!=1.4.6,>=1.1->zope.sqlalchemy->apex) (3.0.3)\n",
      "Requirement already satisfied: PasteDeploy>=2.0 in ./.venv/lib/python3.10/site-packages (from plaster-pastedeploy->pyramid>1.1.2->apex) (3.1.0)\n",
      "Requirement already satisfied: defusedxml in ./.venv/lib/python3.10/site-packages (from python3-openid->velruse>=1.0.3->apex) (0.7.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in ./.venv/lib/python3.10/site-packages (from requests-oauthlib->velruse>=1.0.3->apex) (3.2.2)\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'UnencryptedCookieSessionFactoryConfig' from 'pyramid.session' (unknown location)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 19\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# !pip install git+https://github.com/NVIDIA/NeMo.git\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# !pip install hydra-core\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# !pip install pytorch_lightning\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# !pip install sacrebleu\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# !pip install rouge_score\u001b[39;00m\n\u001b[1;32m     18\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpip install apex\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnemo\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcollections\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnlp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TokenClassificationModel\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# list available pre-trained models\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m TokenClassificationModel\u001b[38;5;241m.\u001b[39mlist_available_models():\n",
      "File \u001b[0;32m~/Introduction to Transformer-Based Natural Language Processing/.venv/lib/python3.10/site-packages/nemo/collections/nlp/__init__.py:15\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright (c) 2020, NVIDIA CORPORATION.  All rights reserved.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnemo\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcollections\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnlp\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m data, losses, models, modules\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnemo\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpackage_info\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Set collection version equal to NeMo version.\u001b[39;00m\n",
      "File \u001b[0;32m~/Introduction to Transformer-Based Natural Language Processing/.venv/lib/python3.10/site-packages/nemo/collections/nlp/data/__init__.py:16\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright (c) 2020, NVIDIA CORPORATION.  All rights reserved.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnemo\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcollections\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnlp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnemo\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcollections\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnlp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mentity_linking\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mentity_linking_dataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m EntityLinkingDataset\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnemo\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcollections\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnlp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minformation_retrieval\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minformation_retrieval_dataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     18\u001b[0m     BertInformationRetrievalDataset,\n\u001b[1;32m     19\u001b[0m )\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnemo\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcollections\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnlp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlanguage_modeling\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01ml2r_lm_dataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     21\u001b[0m     L2RLanguageModelingDataset,\n\u001b[1;32m     22\u001b[0m     TarredL2RLanguageModelingDataset,\n\u001b[1;32m     23\u001b[0m )\n",
      "File \u001b[0;32m~/Introduction to Transformer-Based Natural Language Processing/.venv/lib/python3.10/site-packages/nemo/collections/nlp/data/entity_linking/__init__.py:15\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright (c) 2021, NVIDIA CORPORATION.  All rights reserved.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnemo\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcollections\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnlp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mentity_linking\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mentity_linking_dataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m EntityLinkingDataset\n",
      "File \u001b[0;32m~/Introduction to Transformer-Based Natural Language Processing/.venv/lib/python3.10/site-packages/nemo/collections/nlp/data/entity_linking/entity_linking_dataset.py:22\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnemo\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcollections\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnlp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_utils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_preprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m find_newlines, load_data_indices\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnemo\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclasses\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnemo\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m logging\n\u001b[1;32m     25\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEntityLinkingDataset\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/Introduction to Transformer-Based Natural Language Processing/.venv/lib/python3.10/site-packages/nemo/core/__init__.py:16\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright (c) 2020, NVIDIA CORPORATION.  All rights reserved.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnemo\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mneural_types\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnemo\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclasses\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[0;32m~/Introduction to Transformer-Based Natural Language Processing/.venv/lib/python3.10/site-packages/nemo/core/classes/__init__.py:33\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnemo\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclasses\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mloss\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Loss\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnemo\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclasses\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmixins\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m access_mixins, adapter_mixins, hf_io_mixin\n\u001b[0;32m---> 33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnemo\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclasses\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodelPT\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ModelPT\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnemo\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclasses\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodule\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NeuralModule\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnemo\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m exceptions\n",
      "File \u001b[0;32m~/Introduction to Transformer-Based Natural Language Processing/.venv/lib/python3.10/site-packages/nemo/core/classes/modelPT.py:43\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpytorch_lightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutilities\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m model_summary, rank_zero_only\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnemo\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m package_info\n\u001b[0;32m---> 43\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnemo\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m optim\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnemo\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclasses\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Model\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnemo\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconnectors\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msave_restore_connector\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SaveRestoreConnector\n",
      "File \u001b[0;32m~/Introduction to Transformer-Based Natural Language Processing/.venv/lib/python3.10/site-packages/nemo/core/optim/__init__.py:34\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnemo\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptim\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnovograd\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Novograd\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnemo\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptim\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptimizer_with_main_params\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MainParamsOptimizerWrapper\n\u001b[0;32m---> 34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnemo\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptim\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptimizers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_optimizer, parse_optimizer_args, register_optimizer\n",
      "File \u001b[0;32m~/Introduction to Transformer-Based Natural Language Processing/.venv/lib/python3.10/site-packages/nemo/core/optim/optimizers.py:48\u001b[0m\n\u001b[1;32m     33\u001b[0m AVAILABLE_OPTIMIZERS \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msgd\u001b[39m\u001b[38;5;124m'\u001b[39m: optim\u001b[38;5;241m.\u001b[39mSGD,\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m: optim\u001b[38;5;241m.\u001b[39mAdam,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124madan\u001b[39m\u001b[38;5;124m'\u001b[39m: Adan,\n\u001b[1;32m     45\u001b[0m }\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 48\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mapex\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptimizers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FusedAdam, FusedLAMB\n\u001b[1;32m     50\u001b[0m     HAVE_APEX \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     52\u001b[0m     AVAILABLE_OPTIMIZERS[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlamb\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m FusedLAMB\n",
      "File \u001b[0;32m~/Introduction to Transformer-Based Natural Language Processing/.venv/lib/python3.10/site-packages/apex/__init__.py:13\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyramid\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minterfaces\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (IAuthenticationPolicy,\n\u001b[1;32m     10\u001b[0m                                 IAuthorizationPolicy,\n\u001b[1;32m     11\u001b[0m                                 ISessionFactory)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyramid\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msecurity\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NO_PERMISSION_REQUIRED\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyramid\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msession\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m UnencryptedCookieSessionFactoryConfig\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyramid\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msettings\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m asbool\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mapex\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (ApexAuthSecret,\n\u001b[1;32m     17\u001b[0m                              ApexSessionSecret)\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'UnencryptedCookieSessionFactoryConfig' from 'pyramid.session' (unknown location)"
     ]
    }
   ],
   "source": [
    "# !pip install git+https://github.com/NVIDIA/NeMo.git\n",
    "# !pip install hydra-core\n",
    "# !pip install pytorch_lightning\n",
    "# !pip install einops\n",
    "# !pip install transformers\n",
    "# !pip install sentencepiece\n",
    "# !pip install braceexpand\n",
    "# !pip install webdataset\n",
    "# !pip install h5py\n",
    "# !pip install ijson\n",
    "# !pip install pandas\n",
    "# !pip install sacremoses\n",
    "# !pip install matplotlib\n",
    "# !pip install megatron\n",
    "# !pip install megatron.core\n",
    "# !pip install sacrebleu\n",
    "# !pip install rouge_score\n",
    "!pip install apex\n",
    "from nemo.collections.nlp.models import TokenClassificationModel\n",
    "\n",
    "# list available pre-trained models\n",
    "for model in TokenClassificationModel.list_available_models():\n",
    "    print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225bd9c3",
   "metadata": {},
   "source": [
    "_Note:_ These are models trained for token classification. To get a list of all supported models, use `nemo.collections.nlp.modules.get_pretrained_lm_models_list(include_external=True)`. The list of pre-trained models is expected to change as they become available. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836a6a99",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# download and load the pre-trained BERT-based model\n",
    "pretrained_ner_model=TokenClassificationModel.from_pretrained(\"ner_en_bert\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44a9806",
   "metadata": {},
   "source": [
    "### Make Predictions ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8962e83a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pretrained_ner_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 8\u001b[0m\n\u001b[1;32m      2\u001b[0m queries\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwe bought four shirts from the nvidia gear store in santa clara.\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNvidia is a company.\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      5\u001b[0m ]\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# make sample predictions\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m results\u001b[38;5;241m=\u001b[39m\u001b[43mpretrained_ner_model\u001b[49m\u001b[38;5;241m.\u001b[39madd_predictions(queries)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# show predictions\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m query, result \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(queries, results):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pretrained_ner_model' is not defined"
     ]
    }
   ],
   "source": [
    "# define the list of queries for inference\n",
    "queries=[\n",
    "    'we bought four shirts from the nvidia gear store in santa clara.',\n",
    "    'Nvidia is a company.',\n",
    "]\n",
    "\n",
    "# make sample predictions\n",
    "results=pretrained_ner_model.add_predictions(queries)\n",
    "\n",
    "# show predictions\n",
    "for query, result in zip(queries, results):\n",
    "    print(f'Query : {query}')\n",
    "    print(f'Result: {result.strip()}\\n')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14841924",
   "metadata": {},
   "source": [
    "### Evaluate Predictions ###\n",
    "\n",
    "To see how the model performs, we can generate predictions similar to the way we did it before and compare it with the labels. Alternatively, the `evaluate_from_file()` method enables us to evaluate the model given `text_file` and `labels_file`. Optionally, you can use the `add_confusion_matrix` to get a visual representation of the model performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065f4dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a subset of our dev data\n",
    "!head -n 100 $DATA_DIR/text_dev.txt > $DATA_DIR/sample_text_dev.txt\n",
    "!head -n 100 $DATA_DIR/labels_dev.txt > $DATA_DIR/sample_labels_dev.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c2e358",
   "metadata": {},
   "source": [
    "Now, let's generate predictions for the provided text file. If labels file is also specified, the model will evaluate the predictions and plot confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1364fc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "WORK_DIR = \"WORK_DIR\"\n",
    "\n",
    "# evaluate model performance on sample\n",
    "pretrained_ner_model.evaluate_from_file(\n",
    "    text_file=os.path.join(DATA_DIR, 'sample_text_dev.txt'),\n",
    "    labels_file=os.path.join(DATA_DIR, 'sample_labels_dev.txt'),\n",
    "    output_dir=WORK_DIR,\n",
    "    add_confusion_matrix=True,\n",
    "    normalize_confusion_matrix=True,\n",
    "    batch_size=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f5ae86",
   "metadata": {},
   "source": [
    "## Fine-Tune a Pre-Trained Model ##\n",
    "\n",
    "Without specifying configuration file, NeMo will use the default configurations for the model and trainer. When fine-tuning a pre-trained NER model, we need to setup training and evaluation data before training, the dataset directory is the only required argument if the files names are `labels_dev.txt`, `labels_train.txt`, `text_dev.txt`, and `text_train.txt`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668e4efb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "\n",
    "# setup the data dir to get class weights statistics\n",
    "pretrained_ner_model.update_data_dir(DATA_DIR)\n",
    "\n",
    "# setup train and validation Pytorch DataLoaders\n",
    "pretrained_ner_model.setup_training_data()\n",
    "pretrained_ner_model.setup_validation_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0316a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up loss\n",
    "pretrained_ner_model.setup_loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09865519",
   "metadata": {},
   "source": [
    "_Note:_ Use `class_balancing='weighted_loss'` if you want to add class weights to the `CrossEntropyLoss`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7fef0a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create a PyTorch Lightning trainer and call `fit` again\n",
    "fast_dev_run=True\n",
    "trainer=pl.Trainer(devices=1, accelerator='gpu', fast_dev_run=fast_dev_run)\n",
    "trainer.fit(pretrained_ner_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45ad95a",
   "metadata": {},
   "source": [
    "_Note:_ When training a model, we can set up the model (and trainer) using a configuration file. This is not needed since the task we are performing is the same as the pre-trained model. We will train a custom token classification model in the next notebook, which will require using a configuration file. Furthermore, we are setting `fast_dev_run` to `True` for this demonstration so the trainer will run 1 training batch and 1 validation batch. For actual model training, disable the flag. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46020a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# evaluate model performance on sample\n",
    "pretrained_ner_model.evaluate_from_file(\n",
    "    text_file=os.path.join(DATA_DIR, 'sample_text_dev.txt'),\n",
    "    labels_file=os.path.join(DATA_DIR, 'sample_labels_dev.txt'),\n",
    "    output_dir=WORK_DIR,\n",
    "    add_confusion_matrix=True,\n",
    "    normalize_confusion_matrix=True,\n",
    "    batch_size=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76ea3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# restart the kernel\n",
    "import IPython\n",
    "\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d398c73b",
   "metadata": {},
   "source": [
    "**Well Done!** When you're ready, let's move to the [next notebook](./02_domain-specific_token_classification_model.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570750c7",
   "metadata": {},
   "source": [
    "![DLI Header](images/DLI_Header.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
