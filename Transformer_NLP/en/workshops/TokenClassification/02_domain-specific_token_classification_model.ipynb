{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6703913c",
   "metadata": {},
   "source": [
    "![DLI Header](images/DLI_Header.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b279b1f",
   "metadata": {},
   "source": [
    "# Token Classification with Large Language Models #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f856867",
   "metadata": {},
   "source": [
    "## 02 - Domain-Specific Token Classification Model ##\n",
    "\n",
    "In this notebook, you will learn to fine-tune a pre-trained language model to perform token classification for specific domains. Specifically, you will develop an NER model that finds disease names in medical disease abstracts. \n",
    "\n",
    "**Table of Contents**<br>\n",
    "This notebook covers the below sections: \n",
    "* Project Overview\n",
    "* Dataset\n",
    "    * Download Data\n",
    "    * Preprocess Data\n",
    "* Fine-Tune a Pre-Trained Model for Custom Domain\n",
    "    * Configuration File\n",
    "    * Download Domain-Specific Pre-Trained Model\n",
    "    * Exercise # 1 - Instantiate Model and Trainer\n",
    "    * Exercise # 2 - Model Training\n",
    "    * Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104c803b",
   "metadata": {},
   "source": [
    "## Project Overview ##\n",
    "\n",
    "<img src='images/workflow.png' width=1080>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fedbeff1",
   "metadata": {},
   "source": [
    "## Dataset ##\n",
    "\n",
    "For this notebook, we're going to use the [NCBI-disease](https://www.ncbi.nlm.nih.gov/CBBresearch/Dogan/DISEASE/) corpus, which is a set of 793 PubMed abstracts, annotated by 14 annotators. The annotations take the form of HTML-style tags inserted into the abstract text using the clearly defined rules. The annotations identify named diseases and can be used to fine-tune a language model to identify disease mentions in future abstracts, *whether those diseases were part of the original training set or not*.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c8dd93",
   "metadata": {},
   "source": [
    "### Download Data ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811c9060",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import wget\n",
    "\n",
    "# set data path\n",
    "DATA_DIR = \"data/NCBI\"\n",
    "os.makedirs(DATA_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66fe7753",
   "metadata": {},
   "source": [
    "Here's an example of what an annotated abstract from the corpus looks like: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6ae82c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open(f'{DATA_DIR}/NCBI_corpus_testing.txt') as f: \n",
    "    sample_text=f.readline()\n",
    "    \n",
    "print(sample_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2227bbad",
   "metadata": {},
   "source": [
    "In this example, we see the following tags within the abstract:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b24371e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# use regular expression to find labels\n",
    "categories=re.findall('<category.*?<\\/category>', sample_text)\n",
    "for sample in categories: \n",
    "    print(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39dfc720",
   "metadata": {},
   "source": [
    "For our purposes, we will consider any identified category (such as \"Modifier\", \"Specific Disease\", and a few others) to generally be a \"disease\".  If you want to see more examples, you can explore the text of the corpus using the file browser to the left, or open files directly: \n",
    "\n",
    "* [data/NCBI/NCBI_corpus_training.txt](data/NCBI/NCBI_corpus_training.txt)\n",
    "* [data/NCBI/NCBI_corpus_testing.txt](data/NCBI/NCBI_corpus_testing.txt)\n",
    "* [data/NCBI/NCBI_corpus_development.txt](data/NCBI/NCBI_corpus_development.txt)\n",
    "\n",
    "We have already derived a dataset from this corpus. For NER, the dataset labels individual words as diseases. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15203e19",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "NER_DATA_DIR = f'{DATA_DIR}/NER'\n",
    "os.makedirs(os.path.join(DATA_DIR, 'NER'), exist_ok=True)\n",
    "\n",
    "# show downloaded files\n",
    "!ls -lh $NER_DATA_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233ae97d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!head $NER_DATA_DIR/train.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068818cb",
   "metadata": {},
   "source": [
    "_Note:_ We can see that the abstract has been broken into sentences. Each sentence is then further parsed into words with labels that correspond to the original HTML-style tags in the corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d184f5aa",
   "metadata": {},
   "source": [
    "### Preprocess Data ###\n",
    "\n",
    "We need to convert these to a format that is compatible with NeMo token classification module. For convenience, we've provided the script for this conversion [here](https://github.com/NVIDIA/NeMo/blob/stable/examples/nlp/token_classification/data/import_from_iob_format.py). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff5ad3b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# invoke the conversion script \n",
    "!python import_from_iob_format.py --data_file=$NER_DATA_DIR/train.tsv\n",
    "!python import_from_iob_format.py --data_file=$NER_DATA_DIR/dev.tsv\n",
    "!python import_from_iob_format.py --data_file=$NER_DATA_DIR/test.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb567629",
   "metadata": {},
   "source": [
    "Recall that the sentences and labels in the NER dataset map to each other with _inside, outside, beginning (IOB)_ tagging. Anything separated by white space is a word, including punctuation. This mechanism can be used in a general way for multiple named entity types:\n",
    "* B-{CHUNK_TYPE} – for the word in the Beginning chunk\n",
    "* I-{CHUNK_TYPE} – for words Inside the chunk\n",
    "* O – Outside any chunk\n",
    "\n",
    "In our case, we are only looking for \"disease\" as our entity (or chunk) type, so we don't need to identify beyond the three classes: I, O, and B.\n",
    "**Three classes**\n",
    "* B - Beginning of disease name\n",
    "* I - Inside word of disease name\n",
    "* O - Outside of all disease names\n",
    "\n",
    "As an example, for the first sentence we have the following mapping: \n",
    "\n",
    "```text\n",
    "Identification of APC2 , a homologue of the adenomatous polyposis coli tumour suppressor .\n",
    "O              O  O    O O O         O  O   B           I         I    I      O          O  \n",
    "```\n",
    "\n",
    "For comparison, the original corpus tags looked like:\n",
    "```html\n",
    "Identification of APC2, a homologue of the <category=\"Modifier\">adenomatous polyposis coli tumour</category> suppressor.\n",
    "```\n",
    "\n",
    "The beginning word of the tagged text, \"adenomatous\", is now IOB-tagged with a **B** (beginning) tag, the other parts of the disease, \"polyposis coli tumour\" tagged with **I** (inside) tags, and everything else tagged as **O** (outside)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025eff7e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# preview dataset\n",
    "!head -n 1 $NER_DATA_DIR/text_train.txt\n",
    "!head -n 1 $NER_DATA_DIR/labels_train.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120817f6",
   "metadata": {},
   "source": [
    "## Fine-Tune a Pre-Trained Model for Custom Domain ##\n",
    "\n",
    "A name entity recognition model is typically comprised of a pre-trained [BERT](https://arxiv.org/pdf/1810.04805.pdf) model followed by a token classification layer. For training, we can use a configuration file to define the model. The configuration (config) file consists of several important sections, including: \n",
    "* **model**: All arguments that are related to the Model - language model, token classifier, optimizer and schedulers, datasets and any other related information\n",
    "* **trainer**: Any argument to be passed to PyTorch Lightning\n",
    "\n",
    "_Note:_ NeMo provides a template for creating the configuration file, which is recommended as a starting point, but you can create your own as long as it follows the required format. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee923d77",
   "metadata": {},
   "source": [
    "### Configuration File ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c6b4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define config path\n",
    "MODEL_CONFIG = \"token_classification_config.yaml\"\n",
    "WORK_DIR = \"WORK_DIR\"\n",
    "os.makedirs(WORK_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ff02ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the model's configuration file \n",
    "BRANCH = 'main'\n",
    "config_dir = WORK_DIR + '/configs/'\n",
    "os.makedirs(config_dir, exist_ok=True)\n",
    "\n",
    "if not os.path.exists(config_dir + MODEL_CONFIG):\n",
    "    print('Downloading config file...')\n",
    "    wget.download(f'https://raw.githubusercontent.com/NVIDIA/NeMo/{BRANCH}/examples/nlp/token_classification/conf/' + MODEL_CONFIG, config_dir)\n",
    "else:\n",
    "    print ('config file already exists')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586b1795",
   "metadata": {},
   "source": [
    "The config file for NER, `token_classification_config.yaml`, specifies model, training, and experiment management details, such as file locations, pretrained models, and hyperparameters. The YAML config file we downloaded provides default values for most of the parameters, but there are a few items that must be specified for this experiment.\n",
    "\n",
    "Each YAML section is a bit easier to view using the `omegaconf` package, which allows you to access and manipulate the configuration keys using a \"dot\" notation. We'll take a look at the details of each section using the `OmegaConf` tool. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e398bb3a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from omegaconf import OmegaConf\n",
    "\n",
    "CONFIG_DIR = \"/dli/task/WORK_DIR/configs\"\n",
    "CONFIG_FILE = \"token_classification_config.yaml\"\n",
    "\n",
    "config=OmegaConf.load(CONFIG_DIR + \"/\" + CONFIG_FILE)\n",
    "\n",
    "# print the entire configuration file\n",
    "print(OmegaConf.to_yaml(config))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e83a7f7f",
   "metadata": {},
   "source": [
    "Notice that some config lines, including `model.dataset.data_dir`, have `???` in place of paths, this means that values for these fields are required to be specified by the user. Details about the model arguments can be found in the [documentation](https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/stable/nlp/token_classification.html#training-the-token-classification-model). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d23f8e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# in this exercise, train and dev datasets are located in the same folder under the default names, \n",
    "# so it is enough to add the path of the data directory to the config\n",
    "config.model.dataset.data_dir = os.path.join(DATA_DIR, 'NER')\n",
    "\n",
    "# print the model section\n",
    "print(OmegaConf.to_yaml(config.model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273b75ac",
   "metadata": {},
   "source": [
    "_Note:_ The required `model.dataset.data_dir` argument (for token classification) has been modified. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b9da20",
   "metadata": {},
   "source": [
    "### Download Domain-Specific Pre-Trained Model ###\n",
    "\n",
    "For this token classification task, we can start with the pre-trained `BioMegatron` language model. The `BioMegatron` model is a domain-specific, BERT-like Megatron-LM model trained on large biomedical text corpus. Since the model was trained on domain-specific text, we can expect to have better performance compared to the general language model for identifying disease. \n",
    "\n",
    "_Note:_ There are alternatives of BioMegatron such as BioBERT. It's worth experimenting with different pre-trained models to find the one that provide optimal performance for a specific task. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4317948f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import dependencies\n",
    "from nemo.collections.nlp.models.language_modeling.megatron_bert_model import MegatronBertModel\n",
    "\n",
    "# list available pre-trained models\n",
    "for model in MegatronBertModel.list_available_models(): \n",
    "    print(model.pretrained_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a619827",
   "metadata": {},
   "source": [
    "To load the pretrained BERT LM model, we change the `model.language_mode` argument in the config as well as a few other arguments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6be94a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the specified above model parameters to the config\n",
    "MODEL_NAME='biomegatron345m_biovocab_30k_cased'\n",
    "# MODEL_NAME='biomegatron-bert-345m-cased'\n",
    "\n",
    "config.model.language_model.lm_checkpoint=None\n",
    "config.model.language_model.pretrained_model_name=MODEL_NAME\n",
    "config.model.tokenizer.tokenizer_name=None\n",
    "\n",
    "# use appropriate configurations based on GPU capacity\n",
    "config.model.dataset_max_seq_length=64\n",
    "config.model.train_ds.batch_size=32\n",
    "config.model.validation_ds.batch_size=32\n",
    "config.model.test_ds.batch_size=32\n",
    "\n",
    "# limit the number of epochs for this demonstration\n",
    "config.trainer.max_epochs=1\n",
    "# config.trainer.precision=16\n",
    "# config.trainer.amp_level='O1'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b6a1a1",
   "metadata": {},
   "source": [
    "_Note:_ Once the `token_classification_config.yaml` file has been loaded into memory, changing the configuration file will require the `config` variable to be re-defined. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee5bda8",
   "metadata": {},
   "source": [
    "Now, we are ready to initialize our model. During the model initialization call, the dataset and data loaders will be prepared for training and evaluation. Also, the pretrained BERT model will be downloaded, which can take up to a few minutes depending on the size of the chosen BERT model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce21a4b",
   "metadata": {},
   "source": [
    "#### Exercise # 1 - Instantiate Model and Trainer ####\n",
    "\n",
    "* Modify the `<FIXME>` to instantiate a `TokenClassificationModel` based on the configuration file and trainer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2947d869",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create trainer and model instances\n",
    "from nemo.collections.nlp.models import TokenClassificationModel\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "trainer=pl.Trainer(**config.trainer)\n",
    "ner_model=TokenClassificationModel(<<<<FIXME>>>>)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "915513c0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "from nemo.collections.nlp.models import TokenClassificationModel\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "trainer=pl.Trainer(**config.trainer)\n",
    "ner_model=TokenClassificationModel(cfg=config.model, trainer=trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9addf5e0",
   "metadata": {},
   "source": [
    "click ... to show solution. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929cdb2e",
   "metadata": {},
   "source": [
    "### Exercise # 2 - Model Training ###\n",
    "\n",
    "* Modify the `<FIXME>` to train the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bea3d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start model training\n",
    "trainer.<<<<FIXME>>>>"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6499a593",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "trainer.fit(ner_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9849a378",
   "metadata": {},
   "source": [
    "click ... to show solution. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "223c4668",
   "metadata": {},
   "source": [
    "### Exercise # 3 - Model Evaluation ###\n",
    "\n",
    "* Modify the `<FIXME>` to evaluate the model. \n",
    "\n",
    "To see how the model performs, we can generate prediction similar to the way we did it before and compare it with the labels. Alternatively, the `evaluate_from_file()` method enables us to evaluate the model given `text_file` and `labels_file`. Optionally, you can use the `add_confusion_matrix` to get a visual representation of the model performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff560da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a subset of our dev data\n",
    "!head -n 100 $NER_DATA_DIR/text_dev.txt > $NER_DATA_DIR/sample_text_dev.txt\n",
    "!head -n 100 $NER_DATA_DIR/labels_dev.txt > $NER_DATA_DIR/sample_labels_dev.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f85ba57",
   "metadata": {},
   "source": [
    "Now, let's generate predictions for the provided text file. If labels file is also specified, the model will evaluate the predictions and plot confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42682692",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate model performance on sample\n",
    "ner_model.<<<<FIXME>>>>"
   ]
  },
  {
   "cell_type": "raw",
   "id": "57edb5a0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "ner_model.half().evaluate_from_file(\n",
    "    text_file=os.path.join(NER_DATA_DIR, 'sample_text_dev.txt'),\n",
    "    labels_file=os.path.join(NER_DATA_DIR, 'sample_labels_dev.txt'),\n",
    "    output_dir=WORK_DIR,\n",
    "    add_confusion_matrix=True,\n",
    "    normalize_confusion_matrix=True,\n",
    "    batch_size=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1785e309",
   "metadata": {},
   "source": [
    "click ... to show solution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce20191d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# restart the kernel\n",
    "import IPython\n",
    "\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1a3ebb",
   "metadata": {},
   "source": [
    "**Well Done!** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95042c70",
   "metadata": {},
   "source": [
    "![DLI Header](images/DLI_Header.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
