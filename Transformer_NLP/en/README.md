Large Language Models (LLMs), or Transformers, have revolutionized the field of natural language processing (NLP). Driven by recent advancements, applications of NLP and generative AI have exploded in the past decade. With the proliferation of applications like chatbots and intelligent virtual assistants, organizations are infusing their businesses with more interactive human-machine experiences. Understanding how Transformer-based large language models (LLMs) can be used to manipulate, analyze, and generate text-based data is essential. Modern pre-trained LLMs can encapsulate the nuance, context, and sophistication of language, just as humans do. When fine-tuned and deployed correctly, developers can use these LLMs to build powerful NLP applications that provide natural and seamless human-computer interactions within chatbots, AI voice agents, and more. In this course, you’ll learn how Transformers are used as the building blocks of modern large language models (LLMs). You’ll then use these models for various NLP tasks, including text classification, named-entity recognition (NER), author attribution, and question answering.

## Learning Objectives
By participating in this is course, you will:

- Learn to describe how Transformers are used as the basic building blocks of modern LLMs for NLP applications
- Understand how Transformer-based LLMs can be used to manipulate, analyze, and generate text-based data
- Leverage pre-trained, modern LLMs to solve various NLP tasks such as token classification, text classification, summarization, and question answering

## Course Details

Prerequisites:

- Basic understanding of deep learning concepts
- Basic understanding of language modeling and Transformers

Tools, libraries, frameworks used: NVIDIA NeMo


## Related Training
Getting Started with Deep Learning